import os
import numpy as np
import sys
import time
import json
import argparse
import uproot
import hist
import warnings
import awkward as ak

from coffea.nanoevents import NanoEventsFactory, NanoAODSchema
from  ZH_0lep_total_processor_v3  import TOTAL_Processor

warnings.filterwarnings("ignore", message="Missing cross-reference index")

# --- Argument parser ---
parser = argparse.ArgumentParser()
parser.add_argument("--json", type=str, required=True, help="Path to JSON file")
parser.add_argument("--job-index", type=int, required=True)
parser.add_argument("--output", type=str, required=True, help="Histogram output ROOT file")
parser.add_argument("--dataset", type=str, required=True, help="Dataset key inside JSON")
parser.add_argument("--bdt_output", type=str, default=None, help="Optional: output file for BDT trees")
args = parser.parse_args()

# --- Load dataset info ---
with open(args.json) as f:
    all_datasets = json.load(f)

if args.dataset not in all_datasets:
    raise ValueError(f"[ERROR] Dataset '{args.dataset}' not found in {args.json}")

dataset = all_datasets[args.dataset]
meta = dataset["metadata"]
files = dataset["files"]

if args.job_index >= len(files):
    raise IndexError(f"[ERROR] job-index {args.job_index} is out of range (0 - {len(files)-1})")

file_to_process = files[args.job_index]
dataset_name = meta["sample"]
xsec = float(meta["xsec"])
nevts = int(meta["nevents"])
########################################################
isMC = bool(meta["isMC"])
is_MVA= True
run_eval= False
#run_eval = not is_MVA  # enable eval only in non-training runs

############################################################################################
print(f"[INFO] Processing file {args.job_index+1}/{len(files)}: {file_to_process}")
print(f"[INFO] Sample: {dataset_name} (xsec={xsec}, nevts={nevts})")

# --- Load NanoAOD events ---
for attempt in range(1, 6):
    try:
        factory = NanoEventsFactory.from_root(
            file_to_process,
            schemaclass=NanoAODSchema,
            uproot_options={"timeout": 300}
        )
        events = factory.events()
        break
    except Exception as e:
        print(f"[WARNING] Attempt {attempt} failed: {e}")
        if attempt == 5:
            print("[ERROR] Max attempts reached. Skipping file.")
            sys.exit(1)
        time.sleep(10)

# --- Split TTbar samples by genTtbarId ---
is_ttbar = dataset_name.startswith("TT")

if is_ttbar  and not is_MVA:
    print("[INFO] TTbar sample detected â€“ splitting into ttLF, ttCC, ttBB")
    gen_id = events.genTtbarId

    masks = {
        "ttLF": (gen_id % 100 == 0),
        "ttCC": (gen_id % 100 >= 41) & (gen_id % 100 <= 45),
        "ttBB": (gen_id % 100 >= 51) & (gen_id % 100 <= 55),
    }

    job_suffix = os.path.basename(args.output).split("_")[-1]

    for flavor, mask in masks.items():
        events_flavor = events[mask]
        n_flavor = len(events_flavor)
        print(f"[INFO] Processing flavor: {flavor} (nEvents: {n_flavor})")

        if n_flavor == 0:
            print(f"[INFO] No events found for {flavor} â€” skipping.")
            continue

        processor_instance = TOTAL_Processor(
            xsec=xsec,
            nevts=nevts,
            isMC=isMC,
            dataset_name=dataset_name,
            is_MVA=False,
            run_eval=True
        )
        output = processor_instance.process(events_flavor)

        
        sample_base = os.path.basename(dataset_name).replace(".root", "").replace("/", "_")
        out_name = f"{sample_base}_{flavor}_{job_suffix}"

        with uproot.recreate(out_name) as rootfile:
            for name, obj in output.items():
                if isinstance(obj, hist.Hist):
                    rootfile[name] = obj.to_numpy()
        print(f"[INFO] Saved histogram ROOT file: {out_name}")

else:
    # --- Normal (non-TTbar) processing ---
    processor_instance = TOTAL_Processor(
        xsec=xsec,
        nevts=nevts,
        isMC=isMC,
        dataset_name=dataset_name,
        is_MVA=True,
        run_eval=False
    )
    output = processor_instance.process(events)

    output_name = args.output
    with uproot.recreate(output_name) as rootfile:
        for name, obj in output.items():
            if isinstance(obj, hist.Hist):
                rootfile[name] = obj.to_numpy()
    print(f"[INFO] Saved histogram ROOT file: {output_name}")

    # --- Save BDT trees ---
    bdt_output_name = args.bdt_output or f"bdt_{os.path.basename(args.output)}"
    tree_data = output.get("trees", None)

    if not tree_data and hasattr(processor_instance, "_trees"):
        tree_data = processor_instance._trees

    if tree_data:
        with uproot.recreate(bdt_output_name) as bdtfile:
            for regime, tree_dict in tree_data.items():
                if not tree_dict:
                    # Define expected variables per regime
                    if regime == "boosted":
                        keys = processor_instance.bdt_eval_boost.var_list
                    elif regime == "resolved":
                        keys = processor_instance.bdt_eval_res.var_list
                    else:
                        keys = ["dummy"]

                    # Write empty tree with correct structure
                    dummy_tree = {k: np.array([], dtype=np.float64) for k in keys}
                    print(f"[INFO] Writing empty tree for regime '{regime}' to maintain hadd compatibility.")
                    bdtfile[regime] = dummy_tree
                    continue

                bdtfile[regime] = tree_dict

        print(f"[INFO] Saved BDT training trees in: {bdt_output_name}")
    else:
        print("[WARNING] No BDT trees found â€” nothing was written to tree output file.")
