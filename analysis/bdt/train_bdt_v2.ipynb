{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7ffa442-6326-4ff4-8121-ca6956338d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Training BDT for regime: resolved\n",
      "[DEBUG] Loaded 122 samples from bdt_ZH12.root with label 1\n",
      "[DEBUG] Loaded 154 samples from bdt_ZH15.root with label 1\n",
      "[DEBUG] Loaded 353 samples from bdt_ZH20.root with label 1\n",
      "[DEBUG] Loaded 879 samples from bdt_ZH25.root with label 1\n",
      "[DEBUG] Loaded 1403 samples from bdt_ZH30.root with label 1\n",
      "[DEBUG] Loaded 1353 samples from bdt_ZH40.root with label 1\n",
      "[DEBUG] Loaded 1428 samples from bdt_ZH50.root with label 1\n",
      "[DEBUG] Loaded 1241 samples from bdt_ZH60.root with label 1\n",
      "[DEBUG] Loaded 10 samples from bdt_QCD200.root with label 0\n",
      "[DEBUG] Loaded 23225 samples from bdt_QCD2000.root with label 0\n",
      "[DEBUG] Loaded 2330 samples from bdt_QCD800.root with label 0\n",
      "[DEBUG] Loaded 89 samples from bdt_QCD400.root with label 0\n",
      "[DEBUG] Loaded 8118 samples from bdt_QCD1200.root with label 0\n",
      "[DEBUG] Loaded 744 samples from bdt_QCD600.root with label 0\n",
      "[DEBUG] Loaded 4747 samples from bdt_QCD1000.root with label 0\n",
      "[DEBUG] Loaded 15505 samples from bdt_QCD1500.root with label 0\n",
      "[DEBUG] Loaded 11959 samples from bdt_TT4Q.root with label 0\n",
      "[DEBUG] Loaded 133132 samples from bdt_TT2L.root with label 0\n",
      "[DEBUG] Loaded 203191 samples from bdt_TTLNu.root with label 0\n",
      "[DEBUG] Loaded 314 samples from bdt_Z2Nu100.root with label 0\n",
      "[DEBUG] Loaded 8277 samples from bdt_Z2Nu1500.root with label 0\n",
      "[DEBUG] Loaded 7529 samples from bdt_Z2Nu200.root with label 0\n",
      "[DEBUG] Loaded 9141 samples from bdt_Z2Nu2500.root with label 0\n",
      "[DEBUG] Loaded 4859 samples from bdt_Z2Nu400.root with label 0\n",
      "[DEBUG] Loaded 4507 samples from bdt_Z2Nu800.root with label 0\n",
      "[DEBUG] Loaded 32 samples from bdt_WLNu2j.root with label 0\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[INFO] Best parameters: {'tree_method': 'hist', 'subsample': 0.8, 'reg_lambda': 1, 'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.08, 'gamma': 0, 'colsample_bytree': 0.8, 'alpha': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/r4g6w3c17mb3lqxdm2kw9wc00000gn/T/ipykernel_39199/4049144274.py:208: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"Importance\", y=\"Feature\", data=importance_df, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] resolved — AUC: 0.998, Max Significance: 23.08\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# --- Config ---\n",
    "input_dir = \"/Users/artemis/Desktop/bdt/bdt_inputs\"\n",
    "output_dir = \"/Users/artemis/Desktop/bdt/bdt_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define signal files per regime\n",
    "signal_files_map = {\n",
    "    \"boosted\": [\n",
    "         \"bdt_ZH12.root\", \"bdt_ZH15.root\"\n",
    "    ],\n",
    "    \"merged\": [\n",
    "        \"bdt_ZH20.root\", \"bdt_ZH25.root\"\n",
    "    ],\n",
    "    \"resolved\": [\n",
    "        \"bdt_ZH12.root\", \"bdt_ZH15.root\",\"bdt_ZH20.root\", \"bdt_ZH25.root\",\"bdt_ZH30.root\", \"bdt_ZH40.root\", \"bdt_ZH50.root\", \"bdt_ZH60.root\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Glob all background files (QCD, TT, Zto)\n",
    "background_patterns = [\"bdt_QCD*.root\", \"bdt_TT*.root\", \"bdt_Z2*.root\",  \"bdt_W*.root\"]\n",
    "background_files = []\n",
    "for pattern in background_patterns:\n",
    "    background_files.extend(glob.glob(os.path.join(input_dir, pattern)))\n",
    "\n",
    "regimes = [ \"resolved\"]\n",
    "\n",
    "# --- Data Loader ---\n",
    "def load_tree_data(filename, regime):\n",
    "    with uproot.open(filename) as f:\n",
    "        if regime not in f:\n",
    "            return None\n",
    "        return f[regime].arrays(library=\"np\")\n",
    "\n",
    "def load_data(files, regime, label):\n",
    "    X_all, y_all, w_all = [], [], []\n",
    "    features_used = []\n",
    "\n",
    "    for f in files:\n",
    "        data = load_tree_data(f, regime)\n",
    "        if data is None:\n",
    "            continue\n",
    "\n",
    "        features = [k for k in data.keys() if k != \"weight\"]\n",
    "        X = np.column_stack([data[k] for k in features])\n",
    "        y = np.full(X.shape[0], label)\n",
    "        w = data[\"weight\"] * 112700\n",
    "\n",
    "        X_all.append(X)\n",
    "        y_all.append(y)\n",
    "        w_all.append(w)\n",
    "        features_used = features\n",
    "\n",
    "        print(f\"[DEBUG] Loaded {X.shape[0]} samples from {os.path.basename(f)} with label {label}\")\n",
    "\n",
    "    if not X_all:\n",
    "        return None, None, None, []\n",
    "\n",
    "    return (\n",
    "        np.concatenate(X_all),\n",
    "        np.concatenate(y_all),\n",
    "        np.concatenate(w_all),\n",
    "        features_used,\n",
    "    )\n",
    "\n",
    "# --- Main Loop ---\n",
    "for regime in regimes:\n",
    "    print(f\"\\n[INFO] Training BDT for regime: {regime}\")\n",
    "\n",
    "    sig_files = [os.path.join(input_dir, fname) for fname in signal_files_map[regime]]\n",
    "    X_sig, y_sig, w_sig, features = load_data(sig_files, regime, 1)\n",
    "    X_bkg, y_bkg, w_bkg, _ = load_data(background_files, regime, 0)\n",
    "    if X_sig is None or X_bkg is None:\n",
    "        print(f\"[WARNING] Missing data for regime {regime}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    X = np.concatenate([X_sig, X_bkg])\n",
    "    y = np.concatenate([y_sig, y_bkg])\n",
    "    w = np.concatenate([w_sig, w_bkg])\n",
    "\n",
    "    X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "        X, y, w, test_size=0.5, stratify=y\n",
    "    )\n",
    "\n",
    "    df_train = pd.DataFrame(X_train, columns=features)\n",
    "    df_test = pd.DataFrame(X_test, columns=features)\n",
    "    df_train['label'] = y_train\n",
    "    df_test['label'] = y_test\n",
    "\n",
    "    # Correlation matrices\n",
    "       # Correlation matrices WITH percentages\n",
    "    for tag, df in zip([\"train_sig\", \"train_bkg\", \"test_sig\", \"test_bkg\"],\n",
    "                       [df_train[df_train.label == 1], df_train[df_train.label == 0],\n",
    "                        df_test[df_test.label == 1], df_test[df_test.label == 0]]):\n",
    "        corr = df[features].corr()\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, cbar=True,\n",
    "                    annot_kws={\"size\": 8})\n",
    "        plt.title(f\"Correlation Matrix ({regime} - {tag})\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/corr_{regime}_{tag}.pdf\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    # Input variable plots\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.figure()\n",
    "        plt.hist(X_sig[:, i], bins=100, density=True, alpha=0.5, label=\"Signal\", color=\"blue\")\n",
    "        plt.hist(X_bkg[:, i], bins=100, density=True, alpha=0.5, label=\"Background\", color=\"red\")\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel(\"Normalized Events\")\n",
    "        plt.title(f\"{feature} ({regime})\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/feature_{regime}_{feature}.pdf\")\n",
    "        plt.close()\n",
    "\n",
    "    # GridSearchCV\n",
    "    param_grid = {\n",
    "        \"max_depth\": [1,2, 3, 4],\n",
    "        \"learning_rate\": [0.01, 0.05,0.08,0.09, 0.1],\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"subsample\": [0.5, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6,0.8, 1.0],\n",
    "        \"tree_method\": [\"hist\"],\n",
    "        \"gamma\": [0, 0.5, 1.0],\n",
    "        \"alpha\": [0, 0.1, 1],\n",
    "        \"reg_lambda\": [1, 2]\n",
    "    }\n",
    "        \n",
    "\n",
    "    model = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\")\n",
    "    #grid = GridSearchCV(model, param_grid, scoring=\"roc_auc\", cv=3, verbose=1, n_jobs=-1)\n",
    "    #grid.fit(X_train, y_train, sample_weight=w_train)\n",
    "    search = RandomizedSearchCV(\n",
    "    model, param_distributions=param_grid,\n",
    "    scoring=\"roc_auc\", cv=3, n_iter=100,  # ← only 100 random combos\n",
    "    verbose=1, n_jobs=-1, random_state=42\n",
    "    )\n",
    "    search.fit(X_train, y_train, sample_weight=w_train)\n",
    "\n",
    "    model = search.best_estimator_\n",
    "    print(f\"[INFO] Best parameters: {search.best_params_}\")\n",
    "    #model = grid.best_estimator_\n",
    "    #print(f\"[INFO] Best parameters: {grid.best_params_}\")\n",
    "\n",
    "    y_pred_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_test  = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # ROC curve\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_train, sample_weight=w_train)\n",
    "    fpr_test, tpr_test, _   = roc_curve(y_test, y_pred_test, sample_weight=w_test)\n",
    "    auc_train = auc(fpr_train, tpr_train)\n",
    "    auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr_train, tpr_train, label=f\"Train AUC = {auc_train:.3f}\")\n",
    "    plt.plot(fpr_test, tpr_test, label=f\"Test AUC = {auc_test:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({regime})\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{output_dir}/roc_{regime}.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "    # Overtraining check\n",
    "    ks_sig = ks_2samp(y_pred_train[y_train == 1], y_pred_test[y_test == 1])\n",
    "    ks_bkg = ks_2samp(y_pred_train[y_train == 0], y_pred_test[y_test == 0])\n",
    "\n",
    "    bins = np.linspace(0, 1, 100)\n",
    "    plt.figure()\n",
    "    plt.hist(y_pred_train[y_train == 1], bins=bins, density=True, histtype='step', label=\"Sig train\", color=\"blue\")\n",
    "    plt.hist(y_pred_test[y_test == 1], bins=bins, density=True, histtype='step', linestyle='--', label=\"Sig test\", color=\"blue\")\n",
    "    plt.hist(y_pred_train[y_train == 0], bins=bins, density=True, histtype='step', label=\"Bkg train\", color=\"red\")\n",
    "    plt.hist(y_pred_test[y_test == 0], bins=bins, density=True, histtype='step', linestyle='--', label=\"Bkg test\", color=\"red\")\n",
    "    plt.title(f\"Overtraining — KS p: Sig={ks_sig.pvalue:.3f}, Bkg={ks_bkg.pvalue:.3f}\")\n",
    "    plt.xlabel(\"BDT Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{output_dir}/overtraining_{regime}.pdf\")\n",
    "    plt.close()\n",
    "    # variable importance\n",
    "        # Variable importance ranking (with actual feature names)\n",
    "    importances = model.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": features,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # Save as bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=importance_df, palette=\"viridis\")\n",
    "    plt.title(f\"Feature Importance ({regime})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/feature_importance_{regime}.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save as CSV (optional)\n",
    "    importance_df.to_csv(f\"{output_dir}/feature_importance_{regime}.csv\", index=False)\n",
    "    \n",
    "    # Efficiency, Rejection, Significance\n",
    "    s_hist, _ = np.histogram(y_pred_test[y_test == 1], bins=bins, weights=w_test[y_test == 1])\n",
    "    b_hist, _ = np.histogram(y_pred_test[y_test == 0], bins=bins, weights=w_test[y_test == 0])\n",
    "    s_cumsum = s_hist[::-1].cumsum()[::-1]\n",
    "    b_cumsum = b_hist[::-1].cumsum()[::-1]\n",
    "\n",
    "    efficiency = s_cumsum / s_cumsum[0]\n",
    "    bkg_eff = b_cumsum / b_cumsum[0]\n",
    "    significance = s_cumsum / np.sqrt(s_cumsum + b_cumsum )\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(bins[:-1], efficiency, label=\"Signal Efficiency\")\n",
    "    plt.plot(bins[:-1], bkg_eff, label=\"Background Rejection\")\n",
    "    plt.plot(bins[:-1], significance, label=\"Significance (S/√(S+B))\")\n",
    "    plt.xlabel(\"BDT Cut\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.title(f\"Performance Metrics vs. BDT Cut ({regime})\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{output_dir}/eff_rej_significance_{regime}.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "    model.save_model(f\"{output_dir}/bdt_model_{regime}.json\")\n",
    "    print(f\"[DONE] {regime} — AUC: {auc_test:.3f}, Max Significance: {np.max(significance):.2f}\")\n",
    "\n",
    "    # Compute max expected signal yield from individual files\n",
    "    max_signal_yield = 0\n",
    "    for f in sig_files:\n",
    "        data = load_tree_data(f, regime)\n",
    "        if data is not None:\n",
    "            yield_estimate = np.sum(data[\"weight\"]) * 112700\n",
    "            if yield_estimate > max_signal_yield:\n",
    "                max_signal_yield = yield_estimate\n",
    "    \n",
    "    # Compute total expected background yield\n",
    "    total_background_yield = 0\n",
    "    for f in background_files:\n",
    "        data = load_tree_data(f, regime)\n",
    "        if data is not None:\n",
    "            total_background_yield += np.sum(data[\"weight\"]) * 112700\n",
    "    \n",
    "    # Compute test set signal and background histograms with rescaled weights\n",
    "    signal_test_weights = w_test[y_test == 1]\n",
    "    background_test_weights = w_test[y_test == 0]\n",
    "    \n",
    "    s_scale = max_signal_yield / np.sum(signal_test_weights)\n",
    "    b_scale = total_background_yield / np.sum(background_test_weights)\n",
    "    \n",
    "    s_hist_sign, _ = np.histogram(y_pred_test[y_test == 1], bins=bins, weights=signal_test_weights * s_scale)\n",
    "    b_hist_sign, _ = np.histogram(y_pred_test[y_test == 0], bins=bins, weights=background_test_weights * b_scale)\n",
    "\n",
    "    s_cumsum_sign = s_hist_sign[::-1].cumsum()[::-1]\n",
    "    b_cumsum_sign = b_hist_sign[::-1].cumsum()[::-1]\n",
    "    \n",
    "    sig_eff_sign = s_cumsum_sign / s_cumsum_sign[0]\n",
    "    bkg_eff_sign = b_cumsum_sign / b_cumsum_sign[0]\n",
    "    sign = s_cumsum_sign / np.sqrt(s_cumsum_sign + b_cumsum_sign)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(bins[:-1],sig_eff_sign, label=\"Signal Efficiency\")\n",
    "    plt.plot(bins[:-1], bkg_eff_sign, label=\"Background Rejection\")\n",
    "    plt.plot(bins[:-1], sign, label=\"Significance (S/√(S+B))\")\n",
    "    plt.xlabel(\"BDT Cut\")\n",
    "    plt.ylabel(\"Metric\")\n",
    "    plt.title(f\"Performance Metrics vs. BDT Cut ({regime})\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"{output_dir}/sign_{regime}.pdf\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7defd3-99bc-4fd2-8400-5f0852dccfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
